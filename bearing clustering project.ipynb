{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bearing Clustering Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING LIBRARIES AND DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data analysis libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "#Visulization and statistics libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import style\n",
    "from scipy import fftpack\n",
    "import seaborn as sns\n",
    "style.use('seaborn')\n",
    "\n",
    "# Model related libraries\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.cluster import Birch\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import AgglomerativeClustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting to display all columns\n",
    "pd.set_option(\"Display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load the data \n",
    "target_set = pd.read_csv(\"bearing_classes.csv\", sep=\";\", skipinitialspace=True)\n",
    "origin_set = pd.read_csv(\"bearing_signals.csv\", skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_set['experiment_id'].nunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_set['bearing_2_id'].nunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_set['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions (To Do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rename the column name using function\n",
    "\"\"\"This function takes dataframe & prefix of the columns.\n",
    "   It needs name of columns from the dataframe and add prefix before the each name of the columns.\n",
    "   It returns dataframe with new column names.\"\"\"\n",
    "def rename_column(df,prefix):\n",
    "    column_name = list(df.columns)\n",
    "    column_name = [prefix + name for name in column_name]\n",
    "    return df.set_axis(column_name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find a frequency with the function\n",
    "\"\"\"This function takes dataframe and number of bearing id.\n",
    "   First create an empty list and apply a for loop with range of length of dataframe.\n",
    "   Make a group as per bearing_2_id of dataframe.\n",
    "   Find the frequency using the fftpack module of scipy library.\n",
    "   Convert this frequency into absulate amplitude values.\n",
    "   Function returns a list of all amplitude values of the freatures.\"\"\"\n",
    "def by_axis_bearing(bearing_feature, i):\n",
    "    max_list = []\n",
    "    for index in range(len(bearing_feature)):\n",
    "        bearing_idx = bearing_feature[index]\n",
    "        df_bearing = bearing_idx[bearing_idx['bearing_2_id'] == i]\n",
    "        fft_values = fftpack.fft(df_bearing)\n",
    "        max_amplitude = np.argmax(np.abs(fft_values))\n",
    "        max_list.append(max_amplitude)\n",
    "    return max_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns with w, rpm, hz\n",
    "\n",
    "def extract_basic_feats(df):\n",
    "    list_of_feats = ['rpm','hz','w','timestamp']\n",
    "    aggregations = ['min', 'mean', 'max']\n",
    "    for feat in list_of_feats :\n",
    "        for aggs in aggregations :\n",
    "            df[feat] = origin_set.groupby(['experiment_id','bearing_2_id'])[feat].transform(aggs)\n",
    "    return df\n",
    "\n",
    "extract_basic_feats(df_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA AND FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To drop some columns from the dataframe\n",
    "df_train = origin_set.drop(['experiment_id','bearing_1_id'], axis=1)\n",
    "df_good = origin_set.drop(['experiment_id', 'bearing_2_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find maximun, minimum, mean, standerd deviation, median, range, kurtosis and skewness\n",
    "# of the data and apply function for rename the column names for each features\n",
    "max_set = df_train.groupby(['bearing_2_id']).max()\n",
    "max_set = rename_column(max_set,\"max_\")\n",
    "min_set = df_train.groupby(['bearing_2_id']).min()\n",
    "min_set = rename_column(min_set,\"min_\")\n",
    "mean_set = df_train.groupby(['bearing_2_id']).mean()\n",
    "mean_set = rename_column(mean_set,\"mean_\")\n",
    "std_set = df_train.groupby(['bearing_2_id']).std()\n",
    "std_set = rename_column(std_set,\"std_\")\n",
    "median_set = df_train.groupby(['bearing_2_id']).median()\n",
    "median_set = rename_column(median_set,\"median_\")\n",
    "range_set = df_train.groupby(['bearing_2_id']).max() - df_train.groupby(['bearing_2_id']).min()\n",
    "range_set = rename_column(range_set,\"range_\")\n",
    "kurtosis_set = df_train.groupby(['bearing_2_id']).apply(pd.DataFrame.kurtosis)\n",
    "kurtosis_set = rename_column(kurtosis_set, 'kurtosis_')\n",
    "skew_set = df_train.groupby(['bearing_2_id']).skew()\n",
    "skew_set = rename_column(skew_set, 'skew_')\n",
    "var_set = df_train.groupby(['bearing_2_id']).var()\n",
    "var_set = rename_column(var_set, 'var_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make dataframe of individual features and make list of new dataframe and list of new column names\n",
    "df_bearing_2_x = df_train.drop(['a1_x','a1_y','a1_z','a2_y','a2_z'], axis = 1)\n",
    "df_bearing_2_y = df_train.drop(['a1_x','a1_y','a1_z','a2_x','a2_z'], axis = 1)\n",
    "df_bearing_2_z = df_train.drop(['a1_x','a1_y','a1_z','a2_x','a2_y'], axis = 1)\n",
    "\n",
    "bearing_feature = [df_bearing_2_x,df_bearing_2_y,df_bearing_2_z]\n",
    "list_column = ['fft_a2_x','fft_a2_y','fft_a2_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the maximum frequency of each accelerations of bearings and make a dataframe\n",
    "number_bearing = df_train['bearing_2_id'].max()\n",
    "\n",
    "max_list = by_axis_bearing(bearing_feature, 1)\n",
    "new_set = pd.DataFrame([max_list], columns=list_column,index=[1])\n",
    "\n",
    "for i in range(2,number_bearing+1):\n",
    "    max_list = by_axis_bearing(bearing_feature,i)\n",
    "    temp_set = pd.DataFrame([max_list],columns=list_column,index=[i])\n",
    "    new_set = new_set.append(temp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([max_set, min_set, mean_set, std_set, median_set, range_set, kurtosis_set, skew_set, new_set, var_set], axis=1)\n",
    "#df['target']= target_set.iloc[1:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To drop some features from the dataframe\n",
    "df1 = df.drop(['max_timestamp','min_timestamp','mean_timestamp','std_timestamp','median_timestamp',\n",
    "              'range_timestamp','kurtosis_bearing_2_id','kurtosis_timestamp','skew_timestamp','var_timestamp',\n",
    "              'max_a1_x', 'max_a1_y', 'max_a1_z','min_a1_x', 'min_a1_y', 'min_a1_z','mean_a1_x',\n",
    "       'mean_a1_y', 'mean_a1_z','std_a1_x','std_a1_y', 'std_a1_z','median_a1_x', 'median_a1_y',\n",
    "       'median_a1_z','range_a1_x','range_a1_y', 'range_a1_z','kurtosis_a1_x', 'kurtosis_a1_y', 'kurtosis_a1_z',\n",
    "               'skew_a1_x', 'skew_a1_y','skew_a1_z','var_a1_x', 'var_a1_y',\n",
    "       'var_a1_z','min_rpm','max_rpm','max_hz','max_w','mean_rpm','median_hz','median_w','range_rpm','range_hz',\n",
    "               'range_w','kurtosis_rpm','kurtosis_hz','kurtosis_w','skew_rpm','skew_hz','skew_w','var_rpm','var_hz',\n",
    "               'min_hz', 'min_w','min_timestamp','var_w', 'std_rpm', 'std_hz', 'std_w', 'mean_hz', 'mean_w','median_rpm'\n",
    "              ], axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing & Standardizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df1_scaled = scaler.fit_transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the whiten function\n",
    "from scipy.cluster.vq import whiten\n",
    "\n",
    "# Use the whiten() function to standardize the data\n",
    "df1_whiten = whiten(df1)\n",
    "print(df1_whiten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "sns.pairplot(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df1)\n",
    "plt.title('Pairplot for the Data', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df1, hue='cluster_labels',height=1.5)\n",
    "g= fig.suptitle(\"Co-relation between features\", y=1.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_2_result = pca_2.fit_transform(df1_scaled)\n",
    "print('Explained variation per principal component: {}'.format(pca_2.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "print('Cumulative variance explained by 2 principal components: {:.2%}'.format(np.sum(pca_2.explained_variance_ratio_)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBOW SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "\n",
    "range_n_clusters = [1, 2, 3, 4, 5, 6]\n",
    "avg_distance=[]\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=42).fit(df1)\n",
    "    avg_distance.append(clusterer.inertia_)\n",
    "\n",
    "style.use(\"fivethirtyeight\")\n",
    "plt.plot(range_n_clusters, avg_distance)\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled data Elbow plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "\n",
    "range_n_clusters = [1, 2, 3, 4, 5, 6]\n",
    "avg_distance=[]\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=42).fit(df1_scaled)\n",
    "    avg_distance.append(clusterer.inertia_)\n",
    "\n",
    "style.use(\"fivethirtyeight\")\n",
    "plt.plot(range_n_clusters, avg_distance)\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dendrogram function\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# Create a dendrogram\n",
    "dn = dendrogram(distance_matrix)\n",
    "\n",
    "# Display the dendogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inertia Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# How to find the best number if Ks?\n",
    "\n",
    "# Running K means with multible Ks\n",
    "\n",
    "no_of_clusters = range(2,20) #[2,3,4,5,6,7,8,9]\n",
    "inertia = []\n",
    "\n",
    "\n",
    "for f in no_of_clusters:\n",
    "    kmeans = KMeans(n_clusters=f, random_state=2)\n",
    "    kmeans = kmeans.fit(df1_metrics)\n",
    "    u = kmeans.inertia_\n",
    "    inertia.append(u)\n",
    "    print(\"The innertia for :\", f, \"Clusters is:\", u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the scree plot for Intertia - elbow method\n",
    "fig, (ax1) = plt.subplots(1, figsize=(16,6))\n",
    "xx = np.arange(len(no_of_clusters))\n",
    "ax1.plot(xx, inertia)\n",
    "ax1.set_xticks(xx)\n",
    "ax1.set_xticklabels(no_of_clusters, rotation='vertical')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia Score')\n",
    "plt.title(\"Inertia Plot per k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Score 2 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.style as style\n",
    "import itertools\n",
    "\n",
    "combi =list(itertools.combinations(df1.columns,2))\n",
    "for feat1,feat2 in combi:\n",
    "    X = df1[[feat1,feat2]].values\n",
    "    range_n_clusters = [2, 3, 4, 5, 6]\n",
    "    silhouette_avg_n_clusters = []\n",
    "\n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(18, 7)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "        # lie within [-0.1, 1]\n",
    "        ax1.set_xlim([-0.1, 1])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        # seed of 10 for reproducibility.\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        silhouette_avg_n_clusters.append(silhouette_avg)\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = \\\n",
    "                sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        # 2nd Plot showing the actual clusters formed\n",
    "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "        ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                    c=colors, edgecolor='k')\n",
    "\n",
    "        # Labeling the clusters\n",
    "        centers = clusterer.cluster_centers_\n",
    "        # Draw white circles at cluster centers\n",
    "        ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "        for i, c in enumerate(centers):\n",
    "            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                        s=50, edgecolor='k')\n",
    "\n",
    "        ax2.set_title(\"The visualization of the clustered data.\")\n",
    "        ax2.set_xlabel(feat1)\n",
    "        ax2.set_ylabel(feat2)\n",
    "\n",
    "        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                      \"with n_clusters = %d\" % n_clusters),\n",
    "                     fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    style.use(\"fivethirtyeight\")\n",
    "    plt.plot(range_n_clusters, silhouette_avg_n_clusters)\n",
    "    plt.xlabel(\"Number of Clusters (k)\")\n",
    "    plt.ylabel(\"silhouette score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score with 3 Feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.style as style\n",
    "import itertools\n",
    "\n",
    "\n",
    "def score_combination(df):\n",
    "    combinations = list(itertools.combinations(df1.columns, 3))\n",
    "\n",
    "    score_dict = dict()\n",
    "    for feature1,feature2,feature3 in combinations:\n",
    "        X = df1[[feature1, feature2,feature3]]\n",
    "        for number_of_clusters in range(2,5):\n",
    "            km = KMeans(random_state=42,n_clusters=number_of_clusters)\n",
    "            km.fit(X)\n",
    "            sil_score= silhouette_score(X, km.labels_)\n",
    "            score_dict[sil_score] = (feature1, feature2, number_of_clusters)\n",
    "    score_dict_keys = list(score_dict.keys())\n",
    "    score_dict_keys = sorted(score_dict_keys,reverse=True)\n",
    "    return score_dict,score_dict_keys\n",
    "\n",
    "def select_plot_score(df1,min_cluster,min_by_cluster,score_dict,score_dict_keys):\n",
    "    features_list = []\n",
    "    \n",
    "    for key in score_dict_keys:\n",
    "        nr_clusters = score_dict[key][2]\n",
    "        if nr_clusters >= min_cluster:\n",
    "            feature1 = score_dict[key][0]\n",
    "            feature2 = score_dict[key][1]\n",
    "            features_list.append(feature1)\n",
    "            features_list.append(feature2)\n",
    "            km = KMeans(random_state=42, n_clusters=nr_clusters)\n",
    "            X = df1[[feature1,feature2]]\n",
    "            km.fit(X)\n",
    "            values,counts = np.unique(km.labels_, return_counts=True)\n",
    "            if sorted(counts)[0]>min_by_cluster:\n",
    "                sns.scatterplot(data=df1, x=feature1, y=feature2, hue=km.labels_)\n",
    "                plt.show()\n",
    "\n",
    "score_dict,score_dict_keys = score_combination(df1)\n",
    "\n",
    "select_plot_score(df,3,30,score_dict,score_dict_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "fig = plt.figure(figsize = (12,12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for cluster in df1.cluster_labels.unique():\n",
    "    ax.scatter(df1.max_a2_x[df1.cluster_labels==cluster],df1.mean_a2_x[df1.cluster_labels==cluster],df1.fft_a2_x[df1.cluster_labels==cluster],label=cluster)\n",
    "\n",
    "ax.set_ylabel(\"MEAN\")\n",
    "ax.set_xlabel(\"MAX ACC (hz)\")\n",
    "ax.set_zlabel(\"FFT\") \n",
    "plt.title('Clustering MAX ACC, STD, FFT on X-Axis', fontsize = 20)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for cluster in df1.cluster_labels.unique():\n",
    "    ax.scatter(df1.max_a2_x[df1.cluster_labels==cluster],df1.std_a2_x[df1.cluster_labels==cluster],df1.fft_a2_x[df1.cluster_labels==cluster],label=cluster)\n",
    "\n",
    "ax.set_xlabel(\"MAX (hz)\")\n",
    "ax.set_ylabel(\"STD\")\n",
    "ax.set_zlabel(\"FFT\") \n",
    "plt.title('Clustering MAX ACC, STD, FFT on X-Axis', fontsize = 20)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## CLUSTERING TECHNIQUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-MEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Skew X-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kmeans and vq functions\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "# Generate cluster centers\n",
    "cluster_centers, distortion = kmeans(df1[['mean_a2_x','skew_a2_x']],3)\n",
    "\n",
    "# Assign cluster labels\n",
    "df1['cluster_labels'], distortion_list = vq(df1[['mean_a2_x','skew_a2_x']], cluster_centers)\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='mean_a2_x', y='skew_a2_x', \n",
    "                hue='cluster_labels', data = df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMEANS - VAR AND RANGE X-AXIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kmeans and vq functions\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "# Generate cluster centers\n",
    "cluster_centers, distortion = kmeans(df1[['var_a2_x','range_a2_x']],3)\n",
    "\n",
    "# Assign cluster labels\n",
    "df1['cluster_labels'], distortion_list = vq(df1[['var_a2_x','range_a2_x']], cluster_centers)\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='var_a2_x', y='range_a2_x', \n",
    "                hue='cluster_labels', data = df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMEANS Max Acc & Mean Acc X-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the kmeans and vq functions\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "# Generate cluster centers\n",
    "cluster_centers, distortion = kmeans(df1[['mean_a2_x','max_a2_x']],3)\n",
    "\n",
    "# Assign cluster labels\n",
    "df1['cluster_labels'], distortion_list = vq(df1[['mean_a2_x','max_a2_x']], cluster_centers)\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='mean_a2_x', y='max_a2_x', \n",
    "                hue='cluster_labels', data = df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans with mean & std X-axis 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kmeans and vq functions\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "# Generate cluster centers\n",
    "cluster_centers, distortion = kmeans(df1[['mean_a2_x','std_a2_x']],3)\n",
    "\n",
    "# Assign cluster labels\n",
    "df1['cluster_labels'], distortion_list = vq(df1[['mean_a2_x','std_a2_x']], cluster_centers)\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='mean_a2_x', y='std_a2_x', \n",
    "                hue='cluster_labels', data = df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means with Mean, Skew, FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kmeans and vq functions\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "# Generate cluster centers\n",
    "cluster_centers, distortion = kmeans(df1[['mean_a2_x','skew_a2_x','fft_a2_x']],3)\n",
    "\n",
    "# Assign cluster labels\n",
    "df1['cluster_labels'], distortion_list = vq(df1[['mean_a2_x','skew_a2_x', 'fft_a2_x']], cluster_centers)\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='mean_a2_x', y='skew_a2_x', \n",
    "                hue='cluster_labels', data = df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "y = kmeans.fit_predict(X[['mean_a2_x','skew_a2_x','fft_a2_x']])\n",
    "\n",
    "X['Cluster'] = y\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "inertia_list = []\n",
    "for state in range(1, 100):\n",
    "    for cluster in range(1,6):\n",
    "    # fit a kmeans object to the dataset\n",
    "        kmeans = KMeans(n_clusters=cluster, init='k-means++', random_state = state).fit(X2)\n",
    "        inertia_list.append(kmeans.inertia_)\n",
    "\n",
    "# clusters is an attribute of the object\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# add cluster index to dataframe\n",
    "        cluster_labels = pd.Series(kmeans.labels_, name='cluster')\n",
    "        X2 = X2.join(cluster_labels.to_frame())\n",
    "    \n",
    "print(\"Min inertia:\", min(inertia_list), \"at random_state = \", inertia_list.index(min(inertia_list)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH FITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# make new dataframe with relevant metrics\n",
    "df1_metrics = df1['mean_a2_x'].to_frame().join(df1['skew_a2_x'].to_frame()).join(df1['median_a2_x'].to_frame())\n",
    "\n",
    "# z-score normalisation\n",
    "df1_metrics_normalized=(df1_metrics-df1_metrics.mean())/df1_metrics.std()\n",
    "df1_metrics_normalized = df1_metrics_normalized.rename(columns={'mean_a2_x': 'mean_zscore',\n",
    "                                                                        'skew_a2_x':'skew_zscore',\n",
    "                                                               'median_a2_x' : 'median_zscore'})\n",
    "\n",
    "# fit a kmeans object to the dataset\n",
    "kmeans = KMeans(n_clusters=2, init='k-means++').fit(df1_metrics_normalized)\n",
    "\n",
    "# clusters is an attribute of the object\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# add cluster index to dataframe\n",
    "cluster_labels = pd.Series(kmeans.labels_, name='cluster')\n",
    "df1_metrics_normalized = df1_metrics_normalized.join(cluster_labels.to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='mean_zscore', y='skew_zscore',  height=10, data=df1_metrics_normalized,\n",
    "           fit_reg=False, hue='cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make new dataframe with relevant metrics\n",
    "df1_metrics = df1['range_a2_y'].to_frame().join(df1['var_a2_y'].to_frame())\n",
    "\n",
    "# z-score normalisation\n",
    "df1_metrics_normalized=(df1_metrics-df1_metrics.mean())/df1_metrics.std()\n",
    "df1_metrics_normalized = df1_metrics_normalized.rename(columns={'range_a2_y': 'range_zscore',\n",
    "                                                                        'var_a2_y':'var_zscore'})\n",
    "\n",
    "# fit a kmeans object to the dataset\n",
    "kmeans = KMeans(n_clusters=2, init='k-means++').fit(df1_metrics_normalized)\n",
    "\n",
    "# clusters is an attribute of the object\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# add cluster index to dataframe\n",
    "cluster_labels = pd.Series(kmeans.labels_, name='cluster')\n",
    "df1_metrics_normalized = df1_metrics_normalized.join(cluster_labels.to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='range_zscore', y='var_zscore',  height=10, data=df1_metrics_normalized,\n",
    "           fit_reg=False, hue='cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AFFINITY PROPAGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# affinity propagation clustering\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X2, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
    "# define the model\n",
    "model = AffinityPropagation(damping=0.9)\n",
    "# fit the model\n",
    "model.fit(X2)\n",
    "# assign a cluster to each example\n",
    "yhat = model.predict(X2)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "    # get row indexes for samples with this cluster\n",
    "    row_ix = where(yhat == cluster)\n",
    "    # create scatter of these samples\n",
    "    pyplot.scatter(X2[row_ix, 0], X2[row_ix, 1])\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agglomerative clustering\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
    "# define the model\n",
    "model = AgglomerativeClustering(n_clusters=2)\n",
    "# fit model and predict clusters\n",
    "yhat = model.fit_predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "    # get row indexes for samples with this cluster\n",
    "    row_ix = where(yhat == cluster)\n",
    "    # create scatter of these samples\n",
    "    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Trying with Dimentionality reduction and then Kmeans\n",
    "\n",
    "n_components = X.shape[1]\n",
    "\n",
    "# Running PCA with all components\n",
    "pca = PCA(n_components=n_components, random_state = 453)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "\n",
    "# Calculating the 95% Variance\n",
    "total_variance = sum(pca.explained_variance_)\n",
    "print(\"Total Variance in our dataset is: \", total_variance)\n",
    "var_95 = total_variance * 0.95\n",
    "print(\"The 95% variance we want to have is: \", var_95)\n",
    "print(\"\")\n",
    "\n",
    "# Creating a df with the components and explained variance\n",
    "a = zip(range(0,n_components), pca.explained_variance_)\n",
    "a = pd.DataFrame(a, columns=[\"PCA Comp\", \"Explained Variance\"])\n",
    "\n",
    "# Trying to hit 95%\n",
    "print(\"Variance explain with 30 n_compononets: \", sum(a[\"Explained Variance\"][0:30]))\n",
    "print(\"Variance explain with 35 n_compononets: \", sum(a[\"Explained Variance\"][0:35]))\n",
    "print(\"Variance explain with 40 n_compononets: \", sum(a[\"Explained Variance\"][0:40]))\n",
    "print(\"Variance explain with 41 n_compononets: \", sum(a[\"Explained Variance\"][0:41]))\n",
    "print(\"Variance explain with 50 n_compononets: \", sum(a[\"Explained Variance\"][0:50]))\n",
    "print(\"Variance explain with 53 n_compononets: \", sum(a[\"Explained Variance\"][0:53]))\n",
    "print(\"Variance explain with 55 n_compononets: \", sum(a[\"Explained Variance\"][0:55]))\n",
    "print(\"Variance explain with 60 n_compononets: \", sum(a[\"Explained Variance\"][0:60]))\n",
    "\n",
    "# Plotting the Data\n",
    "plt.figure(1, figsize=(34, 18))\n",
    "plt.plot(pca.explained_variance_ratio_, linewidth=2, c=\"r\")\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_ratio_')\n",
    "\n",
    "# Plotting line with 95% e.v.\n",
    "plt.axvline(53,linestyle=':', label='n_components - 95% explained', c =\"blue\")\n",
    "plt.legend(prop=dict(size=12))\n",
    "\n",
    "# adding arrow\n",
    "plt.annotate('53 eigenvectors used to explain 95% variance', xy=(53, pca.explained_variance_ratio_[53]), \n",
    "             xytext=(58, pca.explained_variance_ratio_[10]),\n",
    "            arrowprops=dict(facecolor='blue', shrink=0.05))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIRCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.cluster import Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1, clusters = make_blobs(n_samples = 1000, centers = 12, cluster_std = 0.50, random_state = 0)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Birch(branching_factor = 50, n_clusters = 3, threshold = 1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df1[:, 0], df1[:, 1], c = pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=0.4, min_samples=20)\n",
    "db.fit(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = db.fit_predict(df1)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(df1['mean_a2_x'], df1['skew_a2_x'],c=y_pred, cmap='Paired')\n",
    "plt.title(\"Clusters determined by DBSCAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIERARCHICAL CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the fcluster and linkage functions\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "# Use the linkage() function\n",
    "distance_matrix = linkage(df1[['mean_a2_x', 'max_a2_x']], method = 'ward', metric = 'euclidean')\n",
    "\n",
    "# Assign cluster labels\n",
    "df1['cluster_labels'] = fcluster(distance_matrix, 3, criterion='maxclust')\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='mean_a2_x', y='max_a2_x', \n",
    "                hue='cluster_labels', data = df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the fcluster and linkage functions\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "# Use the linkage() function\n",
    "distance_matrix = linkage(df1[['max_a2_x', 'mean_a2_y']], method = 'ward', metric = 'euclidean')\n",
    "\n",
    "# Assign cluster labels\n",
    "df1['cluster_labels'] = fcluster(distance_matrix, 2, criterion='maxclust')\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='max_a2_x', y='mean_a2_x', \n",
    "                hue='cluster_labels', data = df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
